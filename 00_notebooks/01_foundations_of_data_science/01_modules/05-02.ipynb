{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Data Collection & Cleaning Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module consists of 3 parts.\n",
    "\n",
    "- **Part 1** - Data Sources\n",
    "- **Part 2** - Web Scraping\n",
    "- **Part 3** - Data Preparation\n",
    "\n",
    "Each part is provided in a separate file. It is recommended that you follow the order of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#Module-5:-Data-Collection-&-Cleaning-Part-2\" data-toc-modified-id=\"Module-5:-Data-Collection-&-Cleaning-Part-2\">Module 5: Data Collection & Cleaning Part 2</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Table-of-Contents\" data-toc-modified-id=\"Table-of-Contents\">Table of Contents</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Web-Scraping\" data-toc-modified-id=\"Web-Scraping\">Web Scraping</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#HTML-&-XML\" data-toc-modified-id=\"HTML-&-XML\">HTML & XML</a></span>\n",
    "<ul class=\"toc-item\">\n",
    "<li><span><a href=\"#XPath\" data-toc-modified-id=\"XPath\">XPath</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#XSLT\" data-toc-modified-id=\"XSLT\">XSLT</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#JSON\" data-toc-modified-id=\"JSON\">JSON</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#EXERCISE-1:-Scraping-data-on-elected-officials\" data-toc-modified-id=\"EXERCISE-1:-Scraping-data-on-elected-officials\">EXERCISE 1: Scraping data on elected officials</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Scraping-the-Easy-Way\" data-toc-modified-id=\"Scraping-the-Easy-Way\">Scraping the Easy Way</a></span>\n",
    "</li>\n",
    "<li><span><a href=\"#Python-Libraries-for-Web-Scraping\" data-toc-modified-id=\"Python-Libraries-for-Web-Scraping\">Python Libraries for Web Scraping</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><span><a href=\"#References\" data-toc-modified-id=\"References\">References</a></span>\n",
    "</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Now that we have an idea of what the data looks like in terms of form, we can begin scraping the data. Here we cover JSON and XML, as these formats cover the majority of web sources. As a special case we also include HTML.\n",
    "Previously, we showed how to download a CSV file from a website and loaded it into a usable `DataFrame` object. In this section we focus on doing the same so that you'll be able to fully utilize web resources as data.\n",
    "\n",
    "## HTML & XML\n",
    "Python has many libraries for reading and writing data in HTML and XML formats. `lxml` (Behnel, 2018) is a python library that has consistently strong performance in parsing very large files.\n",
    "Lets begin using `lxml` by scraping HTML. \n",
    "\n",
    "**NOTE**: In the following example, we can't use `pandas` to scrape. `pd.read_html(trickySite)` will throw an error.  This is because the `<table>` element isn't used by the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup code and importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "pd.options.display.max_rows = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:03.262Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "trickySite = 'https://xmarquez.github.io/democracyData/reference/pacl.html'\n",
    "# To get started, find the URL you want to extract data from, open it with *requests*.\n",
    "r = requests.get(trickySite, verify=False, )\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? We know we have to extract the data, but wasn't this the entire point of using `pandas`? So that we wouldn't have to do this part?\n",
    "\n",
    "Unfortunately, this part can get very messy, especially for a web-scraping novice. Instead, we will make a simplifying assumption. ***HTML is well formed XML***. If this is true, then we should be able to manipulate and select items as if they are XML. The easiest way to do this is with **XPath** and **XSLT**.\n",
    "\n",
    "### XPath\n",
    "**XML Path Language (XPath)** is a query language for selecting nodes from XML or to compute values from XML. XPath is simply a way to select groups of elements, attributes, text, etc., based on the XML tree structure.  \n",
    "\n",
    "For example, the XPath value `/html/body/a` would grab all `a` elements directly in the `body` element in a root `html` element. However, if an `a` element is embedded inside another element in `body`, then it will not be retrieved. XPath can be thought of as a query language using a syntax similar to file directories. File directories can be absolute or relative. In our example, the path was absolute. If we wanted all `a` elements at any depth inside `body`, then we would use `/html/body//a` as our XPath.\n",
    "\n",
    "Generally, XPath is used with XML. Since XML defines other formats, XPath is a very useful technology to know when dealing with any markup language. It serves the exact same purpose as Regular Expressions, except for matching tree structures and branches in XML.\n",
    "\n",
    "### XSLT\n",
    "\n",
    "**eXtensible Stylesheet Language Transformations (XSLT)** is best described as a way to transform, merge, join, and generally perform operations with XML. The use-case of XSLT is to provide a way to rewrite XML data into other formats. XSLT is a subset of XSL, a stylesheet language. Yet, in order for XSLT to remain general, nothing technology specific can be used. This results in a language specifically designed for expressing transformations concisely for XML. \n",
    "\n",
    "**NOTE**: XSLT is itself expressed as an XML data format.\n",
    "\n",
    "You may notice that XPath is the equivalent to a CSS selector. Both need to find groups of elements and then apply their respective snippets of stylesheet code onto them.\n",
    "\n",
    "Consider the following benefits of these technologies so far:\n",
    "\n",
    "* XML data is guaranteed to be cleanly parsed. \n",
    "* XPath ensures subsets of documents can be retrieved without problems. \n",
    "* XSLT ensures elements can be rewritten from XML input to any output. \n",
    "\n",
    "***Thus XML, XPath, and XSLT becomes a powerful generic data manipulation pipeline.*** But more importantly, it allows us to transform the elements we wish to extract into an HTML table, while ignoring all other elements. We are now able to do all of this in very few lines of code \\cite{Behnel2018}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:03.671Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lxml\n",
    "import lxml.etree\n",
    "\"\"\"\n",
    "1. find the root\n",
    "2. match on all `dl` tags and begin producing a `table` of them\n",
    "3. match on all `dd` and `dt` tags and \n",
    "  * place them into their own rows (<tr> tags)\n",
    "    * inside their own cells (<td> tags)\n",
    "\"\"\"\n",
    "xslt_root = lxml.etree.XML('''\\\n",
    " <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n",
    "     <xsl:template match=\"/\">\n",
    "        <html><body>\n",
    "             <xsl:for-each select=\".//dl\">\n",
    "                 <table>\n",
    "                     <tr>\n",
    "                         <xsl:for-each select=\".//dd\">\n",
    "                         <td>\n",
    "                             <xsl:value-of select=\".\" />\n",
    "                         </td>\n",
    "                         </xsl:for-each>\n",
    "                     </tr>\n",
    "                     <tr>\n",
    "                         <xsl:for-each select=\".//dt\">\n",
    "                             <td>\n",
    "                                 <xsl:value-of select=\".\" />\n",
    "                             </td>\n",
    "                         </xsl:for-each>\n",
    "                     </tr>\n",
    "                 </table>\n",
    "             </xsl:for-each>\n",
    "         </body></html>\n",
    "     </xsl:template>\n",
    " </xsl:stylesheet>''')\n",
    "\n",
    "# compile it from the XSLT\n",
    "transform = lxml.etree.XSLT(xslt_root)\n",
    "\n",
    "# apply the XSLT transformation to an HTML compiled representation of the requested page\n",
    "tree_to_scrape = transform(lxml.etree.HTML(r.content))\n",
    "\n",
    "## Return string of output. Uncomment to view output\n",
    "# str(tree_to_scrape)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:03.742Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>order</td>\n",
       "      <td>Sequential numbering of rows (1 through 9159)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pacl_country</td>\n",
       "      <td>String country identifier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year</td>\n",
       "      <td>Calendar year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>agedem</td>\n",
       "      <td>Age in years of the current regime as classifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>agereg</td>\n",
       "      <td>Age in years of the current regime as classifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>stra</td>\n",
       "      <td>Sum of past transitions to authoritarianism in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1                                                  0\n",
       "0          order      Sequential numbering of rows (1 through 9159)\n",
       "1   pacl_country                         String country identifier.\n",
       "2           year                                      Calendar year\n",
       "..           ...                                                ...\n",
       "68        agedem  Age in years of the current regime as classifi...\n",
       "69        agereg  Age in years of the current regime as classifi...\n",
       "70          stra  Sum of past transitions to authoritarianism in...\n",
       "\n",
       "[71 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Convert to string\n",
    "2. Read with pandas\n",
    "3. Grab the first/zeroth table\n",
    "4. Transpose it\n",
    "5. Switch indexing order for columns (optional)\n",
    "\"\"\" \n",
    "firstTable = pd.read_html(str(tree_to_scrape))[0].T[[1,0]]\n",
    "\n",
    "# return pandas `DataFrame` \n",
    "firstTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "\n",
    "**JavaScript Object Notation (JSON)** has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more flexible data format than a tabular text form like CSV, and much smaller in size than XML files. JSON is very nearly valid Python code. \n",
    "\n",
    "To convert a JSON string to Python form, use `json.loads`. `json.dumps` on the other hand converts a Python object back to JSON. Because the JSON structure is unpredictable, how you convert a JSON object or list of objects to a DataFrame or some other data structure for analysis will be up to you. Conveniently, we are able to pass a list of JSON objects to the `DataFrame` constructor and select subsets of fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:04.386Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metcheckData</th>\n",
       "      <th>feedCreation</th>\n",
       "      <th>feedCreator</th>\n",
       "      <th>feedModel</th>\n",
       "      <th>feedModelRun</th>\n",
       "      <th>feedModelRunInitialTime</th>\n",
       "      <th>feedResolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>forecastLocation</th>\n",
       "      <td>{'forecast': [{'temperature': '7', 'dewpoint':...</td>\n",
       "      <td>2024-03-09T01:49:33.00</td>\n",
       "      <td>Metcheck.com</td>\n",
       "      <td>GHX11</td>\n",
       "      <td>21Z</td>\n",
       "      <td>2024-03-08T21:38:43.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       metcheckData  \\\n",
       "forecastLocation  {'forecast': [{'temperature': '7', 'dewpoint':...   \n",
       "\n",
       "                            feedCreation   feedCreator feedModel feedModelRun  \\\n",
       "forecastLocation  2024-03-09T01:49:33.00  Metcheck.com     GHX11          21Z   \n",
       "\n",
       "                 feedModelRunInitialTime  feedResolution  \n",
       "forecastLocation  2024-03-08T21:38:43.00            0.01  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Endpoint for retrieving Yahoo Weather for Toronto, ON. \n",
    "apiEndpoint =  'http://ws1.metcheck.com/ENGINE/v9_0/json.asp?lat=43.7&lon=-79.4&lid=4868&Fc=No'\n",
    "\n",
    "\n",
    "response_json = pd.read_json(apiEndpoint, lines=False)\n",
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get a pandas `DataFrame` from the results and not the response? `results` actually contains a proper python object. So, it can be manually manipulated or we can reapply pandas to `results`.\n",
    "\n",
    "And, this can be repeated. The best way to repeat would be to use the `map()` function and then combine horizontally or vertically depending on the situation. Additionally, it would be preferable to wrap these in a python function to compartmentalize the code used for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:04.789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast</th>\n",
       "      <th>continent</th>\n",
       "      <th>country</th>\n",
       "      <th>location</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'temperature': '7', 'dewpoint': '0', 'rain': ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.7000/-79.4000</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-79.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'temperature': '6', 'dewpoint': '1', 'rain': ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.7000/-79.4000</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-79.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'temperature': '6', 'dewpoint': '2', 'rain': ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.7000/-79.4000</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-79.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>{'temperature': '0', 'dewpoint': '-11', 'rain'...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.7000/-79.4000</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-79.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>{'temperature': '-1', 'dewpoint': '-12', 'rain...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.7000/-79.4000</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-79.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>{'temperature': '-4', 'dewpoint': '-11', 'rain...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43.7000/-79.4000</td>\n",
       "      <td>43.7</td>\n",
       "      <td>-79.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              forecast continent country  \\\n",
       "0    {'temperature': '7', 'dewpoint': '0', 'rain': ...                     \n",
       "1    {'temperature': '6', 'dewpoint': '1', 'rain': ...                     \n",
       "2    {'temperature': '6', 'dewpoint': '2', 'rain': ...                     \n",
       "..                                                 ...       ...     ...   \n",
       "150  {'temperature': '0', 'dewpoint': '-11', 'rain'...                     \n",
       "151  {'temperature': '-1', 'dewpoint': '-12', 'rain...                     \n",
       "152  {'temperature': '-4', 'dewpoint': '-11', 'rain...                     \n",
       "\n",
       "             location  latitude  longitude  timezone  \n",
       "0    43.7000/-79.4000      43.7      -79.4         5  \n",
       "1    43.7000/-79.4000      43.7      -79.4         5  \n",
       "2    43.7000/-79.4000      43.7      -79.4         5  \n",
       "..                ...       ...        ...       ...  \n",
       "150  43.7000/-79.4000      43.7      -79.4         5  \n",
       "151  43.7000/-79.4000      43.7      -79.4         5  \n",
       "152  43.7000/-79.4000      43.7      -79.4         5  \n",
       "\n",
       "[153 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(response_json[\"metcheckData\"][\"forecastLocation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1: Scraping data on elected officials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the *Members of Parliament* in the [Canadian House of Commons](https://www.ourcommons.ca/) (House of Commons, 2018) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Honorific Title</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Constituency</th>\n",
       "      <th>Province / Territory</th>\n",
       "      <th>Political Affiliation</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ziad</td>\n",
       "      <td>Aboultaif</td>\n",
       "      <td>Edmonton Manning</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2021-09-20 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Scott</td>\n",
       "      <td>Aitchison</td>\n",
       "      <td>Parry Sound—Muskoka</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2021-09-20 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Dan</td>\n",
       "      <td>Albas</td>\n",
       "      <td>Central Okanagan—Similkameen—Nicola</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2021-09-20 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bonita</td>\n",
       "      <td>Zarrillo</td>\n",
       "      <td>Port Moody—Coquitlam</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>NDP</td>\n",
       "      <td>2021-09-20 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Zimmer</td>\n",
       "      <td>Prince George—Peace River—Northern Rockies</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>2021-09-20 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sameer</td>\n",
       "      <td>Zuberi</td>\n",
       "      <td>Pierrefonds—Dollard</td>\n",
       "      <td>Quebec</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>2021-09-20 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Honorific Title First Name  Last Name  \\\n",
       "0               NaN       Ziad  Aboultaif   \n",
       "1               NaN      Scott  Aitchison   \n",
       "2               NaN        Dan      Albas   \n",
       "..              ...        ...        ...   \n",
       "333             NaN     Bonita   Zarrillo   \n",
       "334             NaN        Bob     Zimmer   \n",
       "335             NaN     Sameer     Zuberi   \n",
       "\n",
       "                                   Constituency Province / Territory  \\\n",
       "0                              Edmonton Manning              Alberta   \n",
       "1                           Parry Sound—Muskoka              Ontario   \n",
       "2           Central Okanagan—Similkameen—Nicola     British Columbia   \n",
       "..                                          ...                  ...   \n",
       "333                        Port Moody—Coquitlam     British Columbia   \n",
       "334  Prince George—Peace River—Northern Rockies     British Columbia   \n",
       "335                         Pierrefonds—Dollard               Quebec   \n",
       "\n",
       "    Political Affiliation              Start Date  End Date  \n",
       "0            Conservative  2021-09-20 12:00:00 AM       NaN  \n",
       "1            Conservative  2021-09-20 12:00:00 AM       NaN  \n",
       "2            Conservative  2021-09-20 12:00:00 AM       NaN  \n",
       "..                    ...                     ...       ...  \n",
       "333                   NDP  2021-09-20 12:00:00 AM       NaN  \n",
       "334          Conservative  2021-09-20 12:00:00 AM       NaN  \n",
       "335               Liberal  2021-09-20 12:00:00 AM       NaN  \n",
       "\n",
       "[336 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL of the CSV file\n",
    "csv_url = 'https://www.ourcommons.ca/Members/en/search/csv?caucusId=all&province=all&gender=all'\n",
    "\n",
    "# Download and read the CSV file into a DataFrame\n",
    "df = pd.read_csv('https://www.ourcommons.ca/Members/en/search/csv?caucusId=all&province=all&gender=all')\n",
    "\n",
    "# Now, df contains your data, and you can work with it as needed\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿Message</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The paths for the exports have migrated to a n...</td>\n",
       "      <td>www.ourcommons.ca/members/en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Below is a list of links where to find new exp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Search:</td>\n",
       "      <td>www.ourcommons.ca/members/en/search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Party Standings:</td>\n",
       "      <td>www.ourcommons.ca/members/en/party-standings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Election Candidates:</td>\n",
       "      <td>www.ourcommons.ca/members/en/election-candidates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chamber Votes:</td>\n",
       "      <td>www.ourcommons.ca/members/en/votes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ï»¿Message  \\\n",
       "0   The paths for the exports have migrated to a n...   \n",
       "1   Below is a list of links where to find new exp...   \n",
       "2                                            Search:    \n",
       "..                                                ...   \n",
       "8                                   Party Standings:    \n",
       "9                               Election Candidates:    \n",
       "10                                    Chamber Votes:    \n",
       "\n",
       "                                                 URL  \n",
       "0                       www.ourcommons.ca/members/en  \n",
       "1                                                NaN  \n",
       "2                www.ourcommons.ca/members/en/search  \n",
       "..                                               ...  \n",
       "8       www.ourcommons.ca/members/en/party-standings  \n",
       "9   www.ourcommons.ca/members/en/election-candidates  \n",
       "10                www.ourcommons.ca/members/en/votes  \n",
       "\n",
       "[11 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SOLUTION\n",
    "r = requests.get('https://www.ourcommons.ca/Parliamentarians/en/members/export?output=CSV&listSeperator=,')\n",
    "pd.read_csv(filepath_or_buffer=io.StringIO(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Easy Way\n",
    "\n",
    "There is of course nothing wrong with a more interactive or manual approach for webscraping using interactive tools. We only focus on automating the process for repetition. But it should be noted that websites are not static and will often change. For example, people profiles for Members of Parliament change on a regular basis. \n",
    "\n",
    "\n",
    "Below are some alternatives for scraping\n",
    "\n",
    "* [Google Chrome Web Scraper extension](http://webscraper.io/)\n",
    "* [import.io](http://import.io)\n",
    "* [Spooky Stuff](https://github.com/tribbloid/spookystuff)\n",
    "\n",
    "## Python Libraries for Web Scraping\n",
    "There are many libraries that can be used for scraping web resources. We chose to use the `requests` library largely due to its ability to fully interact with Web APIs and its ability to reduce most use-cases to one-line of code in comparison to other libraries. Similarly, we make use of `lxml` due to its support for XML, XPath, and XSLT for easier transformation into a format immediately useable by `pandas`. Below are the libraries in Python relevant for the web scraping domain.\n",
    "\n",
    "* [urllib2](https://docs.python.org/3.6/library/urllib.request.html)\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "  * **NOTE**: `lxml` is used by the library.\n",
    "* [scrapy](https://docs.scrapy.org/en/latest/)\n",
    "  * **NOTE**: library is for crawling entire websites, and not  requesting only one specific Web Resource.\n",
    "* [requests](http://docs.python-requests.org/en/master/)\n",
    "* [RoboBrowser](https://robobrowser.readthedocs.io/en/latest/readme.html)\n",
    "  * For scraping and filling out forms automatically. Does not render dynamic pages.\n",
    "* [lxml](http://lxml.de/)\n",
    "\n",
    "One topic we don't cover are headless web browsers &mdash; which simulate an actual browser &mdash; enabling scraping of the *Document Object Model* and scraping of dynamic web resources.  The following are libraries and tools specifically for addressing this use case.\n",
    "\n",
    "* [splash](https://splash.readthedocs.io/en/stable/)\n",
    "* [selenium](https://www.seleniumhq.org/)\n",
    "* [Splinter](https://splinter.readthedocs.io/en/latest/)\n",
    "* [phantompy](https://phantompy.readthedocs.io/en/latest/)\n",
    "  * [phantomjs](http://phantomjs.org/)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of Part 2**\n",
    "\n",
    "This notebook makes up one part of this module. Now that you have completed this part, please proceed to the next notebook in this module.\n",
    "If you have any questions, please reach out to your peers using the discussion boards. If you and your peers are unable to come to a suitable conclusion, do not hesitate to reach out to your instructor on the designated discussion board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links**\n",
    "\n",
    "* http://blog.miguelgrinberg.com/post/easy-web-scraping-with-python\n",
    "  * About scraping using other python libraries, as well as crawling entire websites.\n",
    "* http://scrapy.org/\n",
    "  * About writing scrapers as configeration files via scrapy.\n",
    "* https://docs.python.org/2/library/urllib2.html\n",
    "  * Documentation for urlib2 library\n",
    "* http://docs.python-requests.org/en/latest/\n",
    "* The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)\n",
    "  * https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n",
    "* http://import.io\n",
    "  * A web-based platform for extracting data from websites without writing any code.\n",
    "* http://www.crummy.com/software/BeautifulSoup/\n",
    "  * Popular alternative to lxml for web/screen scraping\n",
    "* http://pbpython.com/web-scraping-mn-budget.html\n",
    "  * Tutorial using BeautifulSoup with requests library, pandas, numpy and mathplotlib\n",
    "* Python Regular Expressions Cheat Sheet\n",
    "  * https://pycon2016.regex.training/cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Behnel, S. et al, (2018). Xpath and xslt with lxml [online](http://lxml.de/xpathxslt.html)\n",
    "\n",
    "House of Commons, (2018). Members of Parliament. Retrieved Aug 21, 2018 from http://www.ourcommons.ca/Parliamentarians/en/members\n",
    "\n",
    "McKinney, W. (2017). Python for data analysis: Data wrangling with Pandas, NumPy, and IPython (2nd Ed.). O'Reilly Media."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "496px",
    "width": "386px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ec34e64f2b95dc7183eda31f4b1ab04e3712d1753bd3e78e25e6efc4f82ffea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
