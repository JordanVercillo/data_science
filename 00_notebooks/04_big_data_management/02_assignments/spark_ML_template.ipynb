{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRlsCoQWopS2"
   },
   "source": [
    "## Assignment 4 - Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0xzY8BUopS3"
   },
   "source": [
    "## Learning Outcomes\n",
    "In this assignment you will: \n",
    "\n",
    "-  Use ML piplenes\n",
    "-  Improve a Random Forest model\n",
    "-  Perform Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uF923jNSopS4"
   },
   "source": [
    "**Submission Format**\n",
    "\n",
    "- If you used databricks, please submit the **published** notebook link in a word or pdf document. Do not submit HTML, Jupyter notebook, or archive (DBC) formats.\n",
    "- If you used a local instance of spark, please submit a Jupyter notebook.\n",
    "\n",
    "#### Question 1:  (5 marks)\n",
    "\n",
    "In our learning from this module, we have identified a fairly significant link by leveraging the ML pipeline, a more sophisticated model, and better hyperparameter tuning. However these results are still a bit disappointing. With that being said, we're working with very few features and we've likely made some assumptions that just aren't quite valid (like zip code shortening). Also, just because a rich zip code exists doesn't mean that the farmer's market would be held in that zip code too. In fact we might want to start looking at neighboring zip codes or doing some sort of distance measure to predict whether or not there exists a farmer's market in a certain mile radius from a wealthy zip code.\n",
    "\n",
    "With that being said, we've got a lot of other potential features and plenty of other parameters to tune on our random forest so play around with the above pipeline and see if you can improve it further! Note: adding a feaure for the distance measure is just an example and not a mandatory change to improve the model's performance. We also aren't concerned about if the model's perforamnce is actually improved! We simply want to see if changes have been made to the code for possible improvements. \n",
    "\n",
    "Learn mode about the Farmers Markets dataset, here: https://catalog.data.gov/dataset/farmers-markets-directory-and-geographic-data\n",
    "    \n",
    "You may use the same classifier we built in the notebook in this module.\n",
    "\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5915990090493625/2446126855165611/6085673883631125/latest.html\n",
    "\n",
    "#### Question 2 ( 7 marks)\n",
    "\n",
    "\n",
    "Using the Apache Spark ML pipeline, build a model to predict the price of a diamond based on the available features.\n",
    "\n",
    "Read from the following notebook for details about dataset.\n",
    "\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5915990090493625/4396972618536508/6085673883631125/latest.html\n",
    "\n",
    "Note:  \n",
    "- If you receive an R_Squared value that is negative, that is okay. This may occur due to the low sample size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session running in VS Code!\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VSCodeTest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session running in VS Code!\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "taxes2013 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"C:/Users/jverc/Downloads/2013_soi_zipcode_agi.csv\")\n",
    "\n",
    "markets = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"C:/Users/jverc/Downloads/market_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxes2013.createOrReplaceTempView(\"taxes2013\")\n",
    "markets.createOrReplaceTempView(\"markets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------+-------------+------+-------------------+-----------------------+-----------------+--------------+\n",
      "|state|zipcode|single_returns|joint_returns|numdep|total_income_amount|taxable_interest_amount|net_capital_gains|biz_net_income|\n",
      "+-----+-------+--------------+-------------+------+-------------------+-----------------------+-----------------+--------------+\n",
      "|   AL|      0|        488030|       122290|571240|        1.1444868E7|                77952.0|          23583.0|      824487.0|\n",
      "|   AL|      0|        195840|       155230|383240|        1.7810952E7|                81216.0|          54639.0|      252768.0|\n",
      "|   AL|      0|         72710|       146880|189340|        1.6070153E7|                80627.0|          84137.0|      259836.0|\n",
      "|   AL|      0|         24860|       126480|134370|        1.4288572E7|                71086.0|         105947.0|      214668.0|\n",
      "|   AL|      0|         16930|       168170|177800|         2.605392E7|               149150.0|         404166.0|      567439.0|\n",
      "|   AL|      0|          3530|        42190| 48270|        2.0752068E7|               271416.0|        1569967.0|      822565.0|\n",
      "|   AL|   3500|           950|          260|   710|            19851.0|                  183.0|              4.0|        1657.0|\n",
      "|   AL|   3500|           590|          410|   860|            49338.0|                  172.0|             54.0|         788.0|\n",
      "|   AL|   3500|           290|          490|   620|            56170.0|                  185.0|            139.0|         584.0|\n",
      "|   AL|   3500|            90|          490|   530|            52977.0|                   89.0|            173.0|         339.0|\n",
      "|   AL|   3500|            40|          460|   450|            64329.0|                  205.0|            709.0|        1720.0|\n",
      "|   AL|   3500|             0|           40|    30|            15359.0|                  130.0|              0.0|           0.0|\n",
      "|   AL|   3500|           800|          190|   860|            19011.0|                  135.0|             -1.0|         840.0|\n",
      "|   AL|   3500|           360|          300|   850|            36996.0|                   87.0|             42.0|        -193.0|\n",
      "|   AL|   3500|           140|          230|   340|            28664.0|                   95.0|             22.0|         233.0|\n",
      "|   AL|   3500|            50|          170|   200|            19583.0|                   38.0|             62.0|         -48.0|\n",
      "|   AL|   3500|             0|          160|   170|            25136.0|                   67.0|             48.0|          84.0|\n",
      "|   AL|   3500|             0|            0|     0|                0.0|                    0.0|              0.0|           0.0|\n",
      "|   AL|   3500|           250|          110|   200|             5515.0|                   26.0|              0.0|         543.0|\n",
      "|   AL|   3500|           110|          150|   230|            11906.0|                   32.0|              0.0|         188.0|\n",
      "+-----+-------+--------------+-------------+------+-------------------+-----------------------+-----------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_display = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  state, \n",
    "  INT(zipcode / 10) AS zipcode, \n",
    "  CAST(CAST(mars1 AS DOUBLE) AS INT) AS single_returns, \n",
    "  CAST(CAST(mars2 AS DOUBLE) AS INT) AS joint_returns, \n",
    "  CAST(CAST(numdep AS DOUBLE) AS INT) AS numdep, \n",
    "  CAST(A02650 AS DOUBLE) AS total_income_amount,\n",
    "  CAST(A00300 AS DOUBLE) AS taxable_interest_amount,\n",
    "  CAST(A01000 AS DOUBLE) AS net_capital_gains,\n",
    "  CAST(A00900 AS DOUBLE) AS biz_net_income\n",
    "FROM taxes2013\n",
    "\"\"\")\n",
    "df_display.show()\n",
    "\n",
    "df_display.createOrReplaceTempView(\"cleaned_taxes\")\n",
    "spark.catalog.cacheTable(\"cleaned_taxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to a dataset from a table\n",
    "cleanedTaxes = spark.sql(\"SELECT * FROM cleaned_taxes\")\n",
    "\n",
    "summedTaxes = cleanedTaxes.groupBy(\"zipcode\").sum() # because of AGI, where groups income groups are broken out \n",
    "\n",
    "cleanedMarkets = (markets\n",
    "  .selectExpr(\"*\", \"CAST(TRY_CAST(zip AS INT) / 10 AS INT) AS zipcode\") # Needed \n",
    "  .groupBy(\"zipcode\")\n",
    "  .count()\n",
    "  .selectExpr(\"double(count) as count\", \"zipcode as zip\"))\n",
    "#  selectExpr is short for Select Expression - equivalent to what we\n",
    "#  might be doing in SQL SELECT expression\n",
    "\n",
    "joined = (cleanedMarkets.join(summedTaxes, cleanedMarkets.zip == summedTaxes.zipcode, \"outer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|count| zip|\n",
      "+-----+----+\n",
      "|  5.0|4900|\n",
      "|  2.0|7240|\n",
      "|  8.0|4818|\n",
      "|  1.0|9852|\n",
      "|  2.0|5300|\n",
      "|  5.0|2122|\n",
      "|  2.0|9900|\n",
      "|  1.0|8592|\n",
      "|  1.0|1580|\n",
      "|  1.0|3175|\n",
      "|  1.0|7754|\n",
      "|  2.0|6336|\n",
      "|  1.0|7833|\n",
      "|  1.0|7340|\n",
      "|  1.0|6466|\n",
      "|  4.0|6620|\n",
      "|  1.0|1342|\n",
      "|  2.0| 496|\n",
      "|  1.0|5156|\n",
      "|  1.0|4101|\n",
      "+-----+----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "cleanedMarkets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "| count| zip|zipcode|sum(zipcode)|sum(single_returns)|sum(joint_returns)|sum(numdep)|sum(total_income_amount)|sum(taxable_interest_amount)|sum(net_capital_gains)|sum(biz_net_income)|\n",
      "+------+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|1009.0|NULL|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|   0|      0|           0|           66430180|          52885400|   96500590|           9.274122025E9|                  8.271064E7|          3.99567789E8|       3.10024683E8|\n",
      "|   1.0|   3|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   4.0|  60|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|  61|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   2.0|  62|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|  63|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|  65|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   4.0|  66|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   4.0|  67|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   4.0|  68|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   2.0|  70|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|  71|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   2.0|  72|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   2.0|  73|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   3.0|  75|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   2.0|  76|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|  77|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|  78|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "|   1.0|  79|   NULL|        NULL|               NULL|              NULL|       NULL|                    NULL|                        NULL|                  NULL|               NULL|\n",
      "+------+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|count| zip|zipcode|sum(zipcode)|sum(single_returns)|sum(joint_returns)|sum(numdep)|sum(total_income_amount)|sum(taxable_interest_amount)|sum(net_capital_gains)|sum(biz_net_income)|\n",
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|  1.0|1342|   1342|       40260|               4800|              3680|       5450|                475283.0|                      2919.0|                6476.0|            13496.0|\n",
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "\n",
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|count| zip|zipcode|sum(zipcode)|sum(single_returns)|sum(joint_returns)|sum(numdep)|sum(total_income_amount)|sum(taxable_interest_amount)|sum(net_capital_gains)|sum(biz_net_income)|\n",
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "| NULL|NULL|    833|        9996|              14000|              9490|      19940|               1497553.0|                      7747.0|               12788.0|            32485.0|\n",
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matches source data\n",
    "joined.filter(\"zipcode = 1342\").show()\n",
    "joined.filter(\"zipcode = 833\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|count| zip|zipcode|sum(zipcode)|sum(single_returns)|sum(joint_returns)|sum(numdep)|sum(total_income_amount)|sum(taxable_interest_amount)|sum(net_capital_gains)|sum(biz_net_income)|\n",
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|  1.0|1342|   1342|       40260|               4800|              3680|       5450|                475283.0|                      2919.0|                6476.0|            13496.0|\n",
      "+-----+----+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "\n",
      "+-----+---+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|count|zip|zipcode|sum(zipcode)|sum(single_returns)|sum(joint_returns)|sum(numdep)|sum(total_income_amount)|sum(taxable_interest_amount)|sum(net_capital_gains)|sum(biz_net_income)|\n",
      "+-----+---+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|  0.0|  0|    833|        9996|              14000|              9490|      19940|               1497553.0|                      7747.0|               12788.0|            32485.0|\n",
      "+-----+---+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "\n",
      "+------+---+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "| count|zip|zipcode|sum(zipcode)|sum(single_returns)|sum(joint_returns)|sum(numdep)|sum(total_income_amount)|sum(taxable_interest_amount)|sum(net_capital_gains)|sum(biz_net_income)|\n",
      "+------+---+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "|1009.0|  0|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0|  0|      0|           0|           66430180|          52885400|   96500590|           9.274122025E9|                  8.271064E7|          3.99567789E8|       3.10024683E8|\n",
      "|   1.0|  3|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   4.0| 60|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0| 61|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   2.0| 62|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0| 63|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0| 65|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   4.0| 66|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   4.0| 67|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   4.0| 68|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   2.0| 70|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0| 71|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   2.0| 72|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   2.0| 73|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   3.0| 75|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   2.0| 76|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0| 77|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0| 78|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "|   1.0| 79|      0|           0|                  0|                 0|          0|                     0.0|                         0.0|                   0.0|                0.0|\n",
      "+------+---+-------+------------+-------------------+------------------+-----------+------------------------+----------------------------+----------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "prepped = joined.na.fill(0)\n",
    "prepped.filter(\"zipcode = 1342\").show()\n",
    "prepped.filter(\"zipcode = 833\").show()\n",
    "prepped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonFeatureCols = [\"zip\", \"zipcode\", \"count\"]\n",
    "featureCols = [item for item in prepped.columns if item not in nonFeatureCols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler Assembles all of these columns into one single vector. To do this, set the input columns and output column. Then that assembler will be used to transform the prepped data to the final dataset.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = (VectorAssembler()\n",
    "  .setInputCols(featureCols)\n",
    "  .setOutputCol(\"features\"))\n",
    "\n",
    "finalPrep = assembler.transform(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4107\n",
      "1695\n"
     ]
    }
   ],
   "source": [
    "training, test = finalPrep.randomSplit([0.7, 0.3])\n",
    "\n",
    "#  Going to cache the data to make sure things stay snappy!\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "print(training.count()) # Why execute count here??\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing out the model Parameters:\n",
      "--------------------\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.5)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: count)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lrModel = (LinearRegression()\n",
    "  .setLabelCol(\"count\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setElasticNetParam(0.5))\n",
    "\n",
    "print(\"Printing out the model Parameters:\")\n",
    "print(\"-\"*20)\n",
    "print(lrModel.explainParams())\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "lrFitted = lrModel.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----+-----+\n",
      "|    raw_prediction|prediction|count|equal|\n",
      "+------------------+----------+-----+-----+\n",
      "|1.6266480360272162|       2.0|  0.0|    0|\n",
      "|1.3960107124675702|       1.0|  0.0|    0|\n",
      "|1.1748730028702283|       1.0|  0.0|    0|\n",
      "|1.6076730958060064|       2.0|  0.0|    0|\n",
      "|1.6195503737863977|       2.0|  1.0|    0|\n",
      "| 1.662668739464466|       2.0|  1.0|    0|\n",
      "| 1.425376468040827|       1.0|  1.0|    1|\n",
      "|1.4983417646037873|       1.0|  1.0|    1|\n",
      "|1.2356768699980547|       1.0|  1.0|    1|\n",
      "|0.5773431559152427|       1.0|  2.0|    0|\n",
      "|0.7806139194397206|       1.0|  2.0|    0|\n",
      "|1.9457817650198195|       2.0|  5.0|    0|\n",
      "|1.6599200673154881|       2.0|  0.0|    0|\n",
      "|1.5975276374177712|       2.0|  0.0|    0|\n",
      "|1.6380010046941103|       2.0|  1.0|    0|\n",
      "|1.2801867869754961|       1.0|  1.0|    1|\n",
      "|2.6075669587082286|       3.0|  8.0|    0|\n",
      "|1.4584202027423474|       1.0|  0.0|    0|\n",
      "|1.3832767489712159|       1.0|  0.0|    0|\n",
      "|1.5817514555799617|       2.0|  0.0|    0|\n",
      "+------------------+----------+-----+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "holdout = (lrFitted\n",
    "  .transform(test)\n",
    "  .selectExpr(\"prediction as raw_prediction\", \n",
    "    \"double(round(prediction)) as prediction\", \n",
    "    \"count\", \n",
    "    \"\"\"CASE double(round(prediction)) = count \n",
    "  WHEN true then 1\n",
    "  ELSE 0\n",
    "END as equal\"\"\"))\n",
    "\n",
    "holdout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|(sum(equal) / sum(1))|\n",
      "+---------------------+\n",
      "|   0.2176991150442478|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "holdout.selectExpr(\"sum(equal)/sum(1)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.437758112094395\n",
      "MAE: 1.2\n",
      "RMSE: 1.5613321594377012\n",
      "R^2: 0.014553741176523749\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate Mean Squared Error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mse\"\n",
    ")\n",
    "mse = evaluator.evaluate(holdout)\n",
    "\n",
    "mae = evaluator.setMetricName(\"mae\").evaluate(holdout)\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(holdout)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(holdout)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rfModel = (RandomForestRegressor()\n",
    "  .setLabelCol(\"count\")\n",
    "  .setFeaturesCol(\"features\"))\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "  .addGrid(rfModel.maxDepth, [5, 10])\n",
    "  .addGrid(rfModel.numTrees, [20, 60])\n",
    "  .build())\n",
    "# Note, that this parameter grid will take a long time\n",
    "# to run in the community edition due to limited number\n",
    "# of workers available! Be patient for it to run!\n",
    "# If you want it to run faster, remove some of\n",
    "# the above parameters and it'll speed right up!\n",
    "\n",
    "stages = [rfModel]\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "\n",
    "cv = (CrossValidator() # you can feel free to change the number of folds used in cross validation as well\n",
    "  .setEstimator(pipeline) # the estimator can also just be an individual model rather than a pipeline\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setEvaluator(RegressionEvaluator().setLabelCol(\"count\")))\n",
    "\n",
    "pipelineFitted = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Parameters:\n",
      "--------------------\n",
      "RandomForestRegressionModel: uid=RandomForestRegressor_83b41bde753f, numTrees=20, numFeatures=8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestRegressor_83b41bde753f', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'auto',\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance',\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='labelCol', doc='label column name.'): 'count',\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='numTrees', doc='Number of trees to train (>= 1).'): 20,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='seed', doc='random seed.'): 1385906982511651524,\n",
       " Param(parent='RandomForestRegressor_83b41bde753f', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The Best Parameters:\\n--------------------\")\n",
    "print(pipelineFitted.bestModel.stages[0])\n",
    "pipelineFitted.bestModel.stages[0].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_b51eb85a4abd"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineFitted.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+-----+\n",
      "|     raw_prediction|prediction|count|equal|\n",
      "+-------------------+----------+-----+-----+\n",
      "|0.15740984747919035|       0.0|  0.0|    1|\n",
      "| 0.2206896610523093|       0.0|  0.0|    1|\n",
      "| 0.3156566671570846|       0.0|  0.0|    1|\n",
      "|  0.946622624784443|       1.0|  0.0|    0|\n",
      "| 0.9449481125769367|       1.0|  1.0|    1|\n",
      "| 1.0068125603311764|       1.0|  1.0|    1|\n",
      "| 0.9726002700913259|       1.0|  1.0|    1|\n",
      "| 1.1679757038903142|       1.0|  1.0|    1|\n",
      "| 1.4782475185788102|       1.0|  1.0|    1|\n",
      "| 2.3030802616263655|       2.0|  2.0|    1|\n",
      "| 1.9496039240622676|       2.0|  2.0|    1|\n",
      "| 2.9311498276893317|       3.0|  5.0|    0|\n",
      "| 0.9558531423897045|       1.0|  0.0|    0|\n",
      "|0.15740984747919035|       0.0|  0.0|    1|\n",
      "| 1.6158593301353708|       2.0|  1.0|    0|\n",
      "| 0.8279278118275654|       1.0|  1.0|    1|\n",
      "|  1.798522792753974|       2.0|  8.0|    0|\n",
      "| 0.7232243710662721|       1.0|  0.0|    0|\n",
      "| 0.3882577465394846|       0.0|  0.0|    1|\n",
      "| 0.4287309488779423|       0.0|  0.0|    1|\n",
      "+-------------------+----------+-----+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "holdout2 = (pipelineFitted.bestModel\n",
    "  .transform(test)\n",
    "  .selectExpr(\"prediction as raw_prediction\", \n",
    "    \"double(round(prediction)) as prediction\", \n",
    "    \"count\", \n",
    "    \"\"\"CASE double(round(prediction)) = count \n",
    "  WHEN true then 1\n",
    "  ELSE 0\n",
    "END as equal\"\"\"))\n",
    "  \n",
    "holdout2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 7.900294985250737\n",
      "MAE: 1.136873156342183\n",
      "RMSE: 2.810746339542353\n",
      "R^2: -2.1936376698705637\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate Mean Squared Error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"count\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mse\"\n",
    ")\n",
    "mse = evaluator.evaluate(holdout2)\n",
    "\n",
    "mae = evaluator.setMetricName(\"mae\").evaluate(holdout2)\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(holdout2)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(holdout2)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|(sum(equal) / sum(1))|\n",
      "+---------------------+\n",
      "|   0.3817109144542773|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "holdout2.selectExpr(\"sum(equal)/sum(1)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
